import { GoogleGenerativeAI } from "@google/generative-ai";
import { NextRequest, NextResponse } from "next/server";
import knowledgeBase from "@/src/entities/chatbot/model/knowledge-base.json";
import { Redis } from "@upstash/redis";
import { Ratelimit } from "@upstash/ratelimit";

const redis = new Redis({
  url: process.env.KV_REST_API_URL,
  token: process.env.KV_REST_API_TOKEN,
});

const ratelimit = new Ratelimit({
  redis: redis,
  limiter: Ratelimit.slidingWindow(4, "1 m"),
  analytics: true,
});

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY || "");
const MODELS = ["gemini-2.5-flash", "gemini-2.5-flash-lite"];

const OFF_TOPIC_KEYWORDS = [
  "ë‚ ì”¨",
  "ì£¼ì‹",
  "ì½”ì¸",
  "ë¡œë˜",
  "ìš´ì„¸",
  "ìŒì‹",
  "ë§›ì§‘",
  "ë‰´ìŠ¤",
  "ì •ì¹˜",
  "ì—°ì˜ˆ",
  "ê²Œì„ì¶”ì²œ",
  "ì˜¤ëŠ˜ ë­í•´",
  "ì‹¬ì‹¬í•´",
  "ë¹„ì™€",
];

function isOffTopic(message: string): boolean {
  const cleanMessage = message.replace(/\s+/g, "").toLowerCase();
  return OFF_TOPIC_KEYWORDS.some((keyword) => cleanMessage.includes(keyword));
}

function formatKnowledgeBase(): string {
  let formatted = `ì£¼ì œ: ${knowledgeBase.topic}\n`;
  formatted += `ì„¤ëª…: ${knowledgeBase.description}\n\n`;
  formatted += "=== ì œê³µëœ ì •ë³´ ===\n\n";

  knowledgeBase.data.forEach((item, index) => {
    formatted += `${index + 1}. ${item.category}\n`;
    formatted += `${item.content}\n\n`;
  });

  return formatted;
}

function createSystemPrompt(): string {
  const knowledgeContent = formatKnowledgeBase();

  return `
    ë‹¹ì‹ ì€ í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìì˜ í¬íŠ¸í´ë¦¬ì˜¤ ì±—ë´‡(AI ì–´ì‹œìŠ¤í„´íŠ¸)ì…ë‹ˆë‹¤.
    ì•„ë˜ì˜ [ì§€ì‹ ë² ì´ìŠ¤]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    === [ì§€ì‹ ë² ì´ìŠ¤] ===
    ${JSON.stringify(knowledgeBase, null, 2)}
    =====================

    [ë‹µë³€ ê·œì¹™]
    1. **Context Awareness (ë§¥ë½ ì¸ì‹):** - ì‚¬ìš©ìê°€ "ê±°ê¸°", "ê·¸ í”„ë¡œì íŠ¸", "ê·¸ í•™êµ"ì™€ ê°™ì´ ëŒ€ëª…ì‚¬ë¥¼ ì‚¬ìš©í•˜ë©´, **ì´ì „ ëŒ€í™” ë‚´ì—­(messages)**ì„ ë¶„ì„í•˜ì—¬ ë¬´ì—‡ì„ ì§€ì¹­í•˜ëŠ”ì§€ íŒŒì•…í•œ ë’¤ ë‹µë³€í•˜ì„¸ìš”.
    
    2. **Flexible Bridging (ìœ ì—°í•œ ì—°ê²°):**
       - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í¬íŠ¸í´ë¦¬ì˜¤ì— ì§ì ‘ì ìœ¼ë¡œ ëª…ì‹œë˜ì§€ ì•Šì•˜ë”ë¼ë„(ì˜ˆ: í•™êµ ìœ„ì¹˜, ì‚¬ìš© ê¸°ìˆ ì˜ ì¼ë°˜ì ì¸ ì¥ë‹¨ì  ë“±), 
       - ëŒ€í™” íë¦„ìƒ ì‘ì„±ìì˜ ë°°ê²½ì´ë‚˜ ê¸°ìˆ  ìŠ¤íƒê³¼ ì—°ê´€ëœë‹¤ë©´ **ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•´ ê°„ë‹¨íˆ ë‹µë³€í•˜ê³ , ë‹¤ì‹œ í¬íŠ¸í´ë¦¬ì˜¤ ë‚´ìš©ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°**í•˜ì„¸ìš”.
       - ì˜ˆì‹œ: "ìˆ­ì‹¤ëŒ€í•™êµëŠ” ì„œìš¸ ë™ì‘êµ¬ì— ìˆìŠµë‹ˆë‹¤. ì‘ì„±ìëŠ” ê·¸ê³³ì—ì„œ ì»´í“¨í„°ê³µí•™ì„ ì „ê³µí•˜ë©° ì›¹ ê°œë°œ ê¸°ì´ˆë¥¼ ë‹¤ì¡ŒìŠµë‹ˆë‹¤."

    3. **Tone & Manner:**
       - ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ 'í•´ìš”ì²´'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
       - ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§í•˜ê²Œ "ì œ í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ì—ëŠ” ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤."ë¼ê³  ë§í•˜ë˜, ê´€ë ¨ëœ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
       
    4. **Filtering:**
       - ë‚ ì”¨, ì£¼ì‹, ì •ì¹˜ ë“± í¬íŠ¸í´ë¦¬ì˜¤ì™€ ì „í˜€ ë¬´ê´€í•œ ì£¼ì œì— ëŒ€í•´ì„œë§Œ ì •ì¤‘íˆ ê±°ì ˆí•˜ì„¸ìš”.
  `;
}

export async function POST(request: NextRequest) {
  try {
    const ip = request.headers.get("x-forwarded-for") ?? "127.0.0.1";
    const { success } = await ratelimit.limit(ip);

    if (!success) {
      return NextResponse.json(
        {
          response:
            "ë„ˆë¬´ ì§§ì€ ì‹œê°„ì— ë§ì€ ì§ˆë¬¸ì„ í•˜ì…¨ë„¤ìš”! ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ğŸ˜Š",
          success: true,
        },
        { status: 429 }
      );
    }

    const { message } = await request.json();
    if (!message)
      return NextResponse.json({ error: "ë©”ì‹œì§€ ëˆ„ë½" }, { status: 400 });

    if (isOffTopic(message)) {
      return NextResponse.json({
        response:
          "ì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š",
        success: true,
        usedModel: "static-filter",
      });
    }

    const systemInstruction = createSystemPrompt();

    for (const modelName of MODELS) {
      try {
        const model = genAI.getGenerativeModel({
          model: modelName,
          systemInstruction: systemInstruction,
        });
        const result = await model.generateContent(message);
        const text = result.response.text();

        return NextResponse.json({
          response: text,
          success: true,
          usedModel: modelName,
        });
      } catch (error: any) {
        if (error.status === 429) {
          console.warn(`${modelName} í•œë„ ì´ˆê³¼, ë‹¤ìŒ ëª¨ë¸ ì‹œë„...`);
          continue;
        }
        throw error;
      }
    }

    return NextResponse.json(
      { error: "ëª¨ë“  AI ëª¨ë¸ì˜ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤." },
      { status: 429 }
    );
  } catch (error) {
    console.error("Chat Error:", error);
    return NextResponse.json({ error: "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨" }, { status: 500 });
  }
}
