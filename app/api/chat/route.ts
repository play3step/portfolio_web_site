import { GoogleGenerativeAI } from "@google/generative-ai";
import { NextRequest, NextResponse } from "next/server";
import knowledgeBase from "@/src/entities/chatbot/model/knowledge-base.json";
import { Redis } from "@upstash/redis";
import { Ratelimit } from "@upstash/ratelimit";

const redis = new Redis({
  url: process.env.KV_REST_API_URL,
  token: process.env.KV_REST_API_TOKEN,
});

const ratelimit = new Ratelimit({
  redis: redis,
  limiter: Ratelimit.slidingWindow(4, "1 m"),
  analytics: true,
});

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY || "");
const MODELS = ["gemini-2.5-flash", "gemini-2.5-flash-lite"];

const OFF_TOPIC_KEYWORDS = [
  "ë‚ ì”¨",
  "ì£¼ì‹",
  "ì½”ì¸",
  "ë¡œë˜",
  "ìš´ì„¸",
  "ìŒì‹",
  "ë§›ì§‘",
  "ë‰´ìŠ¤",
  "ì •ì¹˜",
  "ì—°ì˜ˆ",
  "ê²Œì„ì¶”ì²œ",
  "ì˜¤ëŠ˜ ë­í•´",
  "ì‹¬ì‹¬í•´",
  "ë¹„ì™€",
];

function isOffTopic(message: string): boolean {
  const cleanMessage = message.replace(/\s+/g, "").toLowerCase();
  return OFF_TOPIC_KEYWORDS.some((keyword) => cleanMessage.includes(keyword));
}

function formatKnowledgeBase(): string {
  let formatted = `ì£¼ì œ: ${knowledgeBase.topic}\n`;
  formatted += `ì„¤ëª…: ${knowledgeBase.description}\n\n`;
  formatted += "=== ì œê³µëœ ì •ë³´ ===\n\n";

  knowledgeBase.data.forEach((item, index) => {
    formatted += `${index + 1}. ${item.category}\n`;
    formatted += `${item.content}\n\n`;
  });

  return formatted;
}

function createSystemPrompt(): string {
  const knowledgeContent = formatKnowledgeBase();
  return `ë‹¹ì‹ ì€ ì œê³µëœ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

${knowledgeContent}

[ë‹µë³€ ê·œì¹™]
1. ìœ„ ì •ë³´ì™€ ì§ì ‘ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
2. ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì„ ë¬¼ì–´ë³´ë©´ "ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ì •ë³´ëŠ” ì œ ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì•ˆë‚´í•©ë‹ˆë‹¤.
3. ì¹œì ˆí•˜ê³  ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.`;
}
export async function POST(request: NextRequest) {
  try {
    const ip = request.headers.get("x-forwarded-for") ?? "127.0.0.1";
    const { success } = await ratelimit.limit(ip);

    if (!success) {
      return NextResponse.json(
        {
          response:
            "ë„ˆë¬´ ì§§ì€ ì‹œê°„ì— ë§ì€ ì§ˆë¬¸ì„ í•˜ì…¨ë„¤ìš”! ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ğŸ˜Š",
          success: true,
        },
        { status: 429 }
      );
    }

    const { message } = await request.json();
    if (!message)
      return NextResponse.json({ error: "ë©”ì‹œì§€ ëˆ„ë½" }, { status: 400 });

    if (isOffTopic(message)) {
      return NextResponse.json({
        response:
          "ì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š",
        success: true,
        usedModel: "static-filter",
      });
    }

    const systemInstruction = createSystemPrompt();

    for (const modelName of MODELS) {
      try {
        const model = genAI.getGenerativeModel({
          model: modelName,
          systemInstruction: systemInstruction,
        });
        const result = await model.generateContent(message);
        const text = result.response.text();

        return NextResponse.json({
          response: text,
          success: true,
          usedModel: modelName,
        });
      } catch (error: any) {
        if (error.status === 429) {
          console.warn(`${modelName} í•œë„ ì´ˆê³¼, ë‹¤ìŒ ëª¨ë¸ ì‹œë„...`);
          continue;
        }
        throw error;
      }
    }

    return NextResponse.json(
      { error: "ëª¨ë“  AI ëª¨ë¸ì˜ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤." },
      { status: 429 }
    );
  } catch (error) {
    console.error("Chat Error:", error);
    return NextResponse.json({ error: "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨" }, { status: 500 });
  }
}
